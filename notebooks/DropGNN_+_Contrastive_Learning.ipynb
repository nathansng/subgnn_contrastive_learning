{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DropGNN + Contrastive Learning Prototyping"
      ],
      "metadata": {
        "id": "Bv46oqe2DpeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Work based off of DropGNN\n",
        "\n",
        "https://arxiv.org/pdf/2111.06283.pdf \n",
        "\n",
        "https://github.com/KarolisMart/DropGNN "
      ],
      "metadata": {
        "id": "gdzu3FAaGB64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import time\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Torch Geometric \n",
        "try: \n",
        "  from torch_geometric.data import DataLoader, Data\n",
        "  from torch_geometric.datasets import TUDataset\n",
        "  from torch_geometric.utils import degree\n",
        "  from torch_geometric.utils.convert import from_networkx\n",
        "  from torch_geometric.nn import GINConv, GINEConv, global_add_pool\n",
        "except ModuleNotFoundError: \n",
        "  !pip install torch_geometric\n",
        "  from torch_geometric.data import DataLoader, Data\n",
        "  from torch_geometric.datasets import TUDataset\n",
        "  from torch_geometric.utils import degree\n",
        "  from torch_geometric.utils.convert import from_networkx\n",
        "  from torch_geometric.nn import GINConv, GINEConv, global_add_pool"
      ],
      "metadata": {
        "id": "cmcqq2oyHCvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Synthetic Datasets"
      ],
      "metadata": {
        "id": "Ecf6XYGdk62p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SymmetrySet:\n",
        "    def __init__(self):\n",
        "        self.hidden_units = 0\n",
        "        self.num_classes = 0\n",
        "        self.num_features = 0\n",
        "        self.num_nodes = 0\n",
        "\n",
        "    def addports(self, data):\n",
        "        data.ports = torch.zeros(data.num_edges, 1)\n",
        "        degs = degree(data.edge_index[0], data.num_nodes, dtype=torch.long) # out degree of all nodes\n",
        "        for n in range(data.num_nodes):\n",
        "            deg = degs[n]\n",
        "            ports = np.random.permutation(int(deg))\n",
        "            for i, neighbor in enumerate(data.edge_index[1][data.edge_index[0]==n]):\n",
        "                nb = int(neighbor)\n",
        "                data.ports[torch.logical_and(data.edge_index[0]==n, data.edge_index[1]==nb), 0] = float(ports[i])\n",
        "        return data\n",
        "\n",
        "    def makefeatures(self, data):\n",
        "        data.x = torch.ones((data.num_nodes, 1))\n",
        "        data.id = torch.tensor(np.random.permutation(np.arange(data.num_nodes))).unsqueeze(1)\n",
        "        return data\n",
        "\n",
        "    def makedata(self):\n",
        "        pass"
      ],
      "metadata": {
        "id": "UqqC1QHsk6ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LimitsOne(SymmetrySet):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden_units = 16\n",
        "        self.num_classes = 2\n",
        "        self.num_features = 4\n",
        "        self.num_nodes = 8\n",
        "        self.graph_class = False\n",
        "\n",
        "    def makedata(self):\n",
        "        n_nodes = 16 # There are two connected components, each with 8 nodes\n",
        "        \n",
        "        ports = [1,1,2,2] * 8\n",
        "        colors = [0, 1, 2, 3] * 4\n",
        "\n",
        "        y = torch.tensor([0]* 8 + [1] * 8)\n",
        "        edge_index = torch.tensor([[0,1,1,2, 2,3,3,0, 4,5,5,6, 6,7,7,4, 8,9,9,10,10,11,11,12,12,13,13,14,14,15,15,8], [1,0,2,1, 3,2,0,3, 5,4,6,5, 7,6,4,7, 9,8,10,9,11,10,12,11,13,12,14,13,15,14,8,15]], dtype=torch.long)\n",
        "        x = torch.zeros((n_nodes, 4))\n",
        "        x[range(n_nodes), colors] = 1\n",
        "        \n",
        "        data = Data(x=x, edge_index=edge_index, y=y)\n",
        "        data.id = torch.tensor(np.random.permutation(np.arange(n_nodes))).unsqueeze(1)\n",
        "        data.ports = torch.tensor(ports).unsqueeze(1)\n",
        "        return [data]"
      ],
      "metadata": {
        "id": "-xgcShp4lS7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph = LimitsOne()"
      ],
      "metadata": {
        "id": "_nZ-aH00k_cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = graph.makedata()[0]\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e__ZBO_k_eD",
        "outputId": "d7a853e7-81f9-48d0-fd08-0f29f1473157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(x=[16, 4], edge_index=[2, 32], y=[16], id=[16, 1], ports=[32, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Node feature matrix (num_nodex x num_node_features)\n",
        "data.x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbJKkHKek_gQ",
        "outputId": "eed02c3b-5ad4-48fb-fe9f-468ec08ea636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 0., 1., 0.],\n",
              "        [0., 0., 0., 1.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 0., 1., 0.],\n",
              "        [0., 0., 0., 1.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 0., 1., 0.],\n",
              "        [0., 0., 0., 1.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 0., 1., 0.],\n",
              "        [0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Graph connectivity in COO format (2 x num_edges)\n",
        "# ith node in first row and second row are connected by an edge\n",
        "data.edge_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fBsqQoIGpb-",
        "outputId": "687b7571-8d76-4a67-db6e-291330d51f24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  1,  1,  2,  2,  3,  3,  0,  4,  5,  5,  6,  6,  7,  7,  4,  8,  9,\n",
              "          9, 10, 10, 11, 11, 12, 12, 13, 13, 14, 14, 15, 15,  8],\n",
              "        [ 1,  0,  2,  1,  3,  2,  0,  3,  5,  4,  6,  5,  7,  6,  4,  7,  9,  8,\n",
              "         10,  9, 11, 10, 12, 11, 13, 12, 14, 13, 15, 14,  8, 15]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Graph/node - level ground truth label \n",
        "data.y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptmFzGeFGpfn",
        "outputId": "1c549a3a-132c-439d-9213-287bc24a12ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Real Dataset"
      ],
      "metadata": {
        "id": "U4VpVUxbJeBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyFilter(object):\n",
        "  def __call__(self, data):\n",
        "    return data.num_nodes <= 70\n",
        "\n",
        "class MyPreTransform(object):\n",
        "  def __call__(self, data):\n",
        "    data.x = degree(data.edge_index[0], data.num_nodes, dtype=torch.long)\n",
        "    data.x = F.one_hot(data.x, num_classes=69).to(torch.float)\n",
        "    return data "
      ],
      "metadata": {
        "id": "kpop90ZxIaZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data \n",
        "path = osp.join(osp.dirname(osp.realpath(\"./\")), 'data', f'IMDB-BINARY')\n",
        "\n",
        "dataset = TUDataset(\n",
        "    path, \n",
        "    name = \"IMDB-BINARY\", \n",
        "    pre_transform = MyPreTransform(), \n",
        "    pre_filter = MyFilter()\n",
        ")"
      ],
      "metadata": {
        "id": "oPb5CIfZJ0TV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1u0WXdtb_Zj",
        "outputId": "db3223f4-c7a7-4096-aa77-046cfad4b008"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMDB-BINARY(996)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Original DropGIN Model\n",
        "\n",
        "TODO:\n",
        "\n",
        "- Need to add function to change whether forward uses contrastive loss head or prediction head \n",
        "- Can use whether model is in training or evaluation to swap heads\n",
        "  - Train Contrastive Model - Uses contrastive loss head and updates base model and contrastive loss head \n",
        "  - Validate Contrastive Model - Uses contrastive loss head and DOES NOT update anything\n",
        "  - Train Prediction Model - Uses prediction loss head and update ONLY prediction loss head\n",
        "  - Testing - Uses prediction loss head and DOES NOT update anything"
      ],
      "metadata": {
        "id": "MtqTi8B8HrCb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1N3ZszgDkOF"
      },
      "outputs": [],
      "source": [
        "class DropGIN(nn.Module):\n",
        "  def __init__(self, hidden_units, dropout, use_aux_loss=True, num_runs=10, p=0.9):\n",
        "    super(DropGIN, self).__init__()\n",
        "\n",
        "    # Set starting parameters for model \n",
        "    num_features = dataset.num_features # Number of initial features \n",
        "    dim = hidden_units # Number of hidden units per layer \n",
        "    self.dropout = dropout # Dropout rate \n",
        "    self.use_aux_loss = use_aux_loss\n",
        "    self.num_runs = num_runs\n",
        "    self.p = p\n",
        "\n",
        "    # Number of layers in model\n",
        "    self.num_layers = 4\n",
        "\n",
        "    self.convs = nn.ModuleList() # Made of num_layers GINConv (linear -> batchnorm1d -> relu -> linear)\n",
        "    self.bns = nn.ModuleList() # Made of num_layers BatchNorm1d \n",
        "    self.fcs = nn.ModuleList() # Made of num_layers + 1 Linear layers mapping from num_features or dim to num_classes\n",
        "\n",
        "    # Add initial layer from num_features to dim \n",
        "    self.convs.append(GINConv(nn.Sequential(nn.Linear(num_features, dim), nn.BatchNorm1d(dim), nn.ReLU(), nn.Linear(dim, dim))))\n",
        "    self.bns.append(nn.BatchNorm1d(dim))\n",
        "    self.fcs.append(nn.Linear(num_features, dataset.num_classes))\n",
        "    self.fcs.append(nn.Linear(dim, dataset.num_classes))\n",
        "\n",
        "    # Add additional layers from dim to dim \n",
        "    for i in range(self.num_layers-1):\n",
        "      self.convs.append(GINConv(nn.Sequential(nn.Linear(dim, dim), nn.BatchNorm1d(dim), nn.ReLU(), nn.Linear(dim, dim))))\n",
        "      self.bns.append(nn.BatchNorm1d(dim))\n",
        "      self.fcs.append(nn.Linear(dim, dataset.num_classes))\n",
        "\n",
        "    # Use aux_loss for dropGNN: made of num_layers + 1 linear layers \n",
        "    # Adds new module list of linear layers \n",
        "    if self.use_aux_loss:\n",
        "      self.aux_fcs = nn.ModuleList()\n",
        "      self.aux_fcs.append(nn.Linear(num_features, dataset.num_classes))\n",
        "      for i in range(self.num_layers):\n",
        "        self.aux_fcs.append(nn.Linear(dim, dataset.num_classes))\n",
        "        \n",
        "  def reset_parameters(self):\n",
        "    # Resets parameters for Linear, GINConv, and BatchNorm1d layers\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Linear):\n",
        "        m.reset_parameters()\n",
        "      elif isinstance(m, GINConv):\n",
        "        m.reset_parameters()\n",
        "      elif isinstance(m, nn.BatchNorm1d):\n",
        "        m.reset_parameters()\n",
        "\n",
        "  def forward(self, data):\n",
        "    # Note: num_runs in DropGNN is average number of nodes in each graph in dataset\n",
        "    # Note: p is 2 * 1 / (1 + gamma), but for this project, p is selected to create augmented views \n",
        "\n",
        "    x = data.x\n",
        "    edge_index = data.edge_index\n",
        "    batch = data.batch\n",
        "            \n",
        "    # Do runs in parallel, by repeating the graphs in the batch\n",
        "    x = x.unsqueeze(0).expand(self.num_runs, -1, -1).clone()   # Flattens features and creates num_runs copy of them \n",
        "    drop = torch.bernoulli(torch.ones([x.size(0), x.size(1)], device=x.device) * self.p).bool()   #  Returns a tensor of randomly dropped nodes based on p (p = probability of not dropping) \n",
        "    x[drop] = torch.zeros([drop.sum().long().item(), x.size(-1)], device=x.device)  # Drop nodes from data  \n",
        "    del drop\n",
        "\n",
        "    # Run augmented subgraph through model \n",
        "    outs = [x]  # Used to store view of x after each layer \n",
        "    x = x.view(-1, x.size(-1))  # Swap dimensions of data features \n",
        "    run_edge_index = edge_index.repeat(1, self.num_runs) + torch.arange(self.num_runs, device=edge_index.device).repeat_interleave(edge_index.size(1)) * (edge_index.max() + 1) # Expand edge_index and augment values\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.convs[i](x, run_edge_index)  # Run node features and edge indices through CONV layer \n",
        "      x = self.bns[i](x)  # Run resulting values through BatchNorm1d\n",
        "      x = F.relu(x)   # Run final values through RELU\n",
        "      outs.append(x.view(self.num_runs, -1, x.size(-1)))    # Rearrange dimensions and append to outs \n",
        "    del run_edge_index\n",
        "\n",
        "    # TODO: Replace this with replacable head for contrastive learning and making predictions \n",
        "    # Aggregates results of runs by summing mean and applying random dropout (not dropping out nodes)\n",
        "    out = None\n",
        "    for i, x in enumerate(outs):\n",
        "      x = x.mean(dim=0)\n",
        "      x = global_add_pool(x, batch)\n",
        "      x = F.dropout(self.fcs[i](x), p=self.dropout, training=self.training)\n",
        "      if out is None:\n",
        "        out = x\n",
        "      else:\n",
        "        out += x\n",
        "\n",
        "    if self.use_aux_loss:\n",
        "      aux_out = torch.zeros(self.num_runs, out.size(0), out.size(1), device=out.device)\n",
        "      run_batch = batch.repeat(self.num_runs) + torch.arange(self.num_runs, device=edge_index.device).repeat_interleave(batch.size(0)) * (batch.max() + 1)\n",
        "      for i, x in enumerate(outs):\n",
        "        x = x.view(-1, x.size(-1))\n",
        "        x = global_add_pool(x, run_batch)\n",
        "        x = x.view(self.num_runs, -1, x.size(-1))\n",
        "        x = F.dropout(self.aux_fcs[i](x), p=self.dropout, training=self.training)\n",
        "        aux_out += x\n",
        "\n",
        "      # Returns probabilities of each class based on aggregated results \n",
        "      return F.log_softmax(out, dim=-1), F.log_softmax(aux_out, dim=-1)\n",
        "    else:\n",
        "      return F.log_softmax(out, dim=-1), 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Modules\n",
        "\n",
        "NOTE: Currently, all training modules are for base DropGNN model\n",
        "\n",
        "TODO: Need 2 training modules\n",
        "\n",
        "1. Uses contrastive learning loss\n",
        "2. Uses same loss as DropGNN "
      ],
      "metadata": {
        "id": "fF6Gp8oIf2yR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, epoch, loader, optimizer):\n",
        "  # Set model to training \n",
        "  model.train()\n",
        "\n",
        "  # Run data through model and update model \n",
        "  loss_all = 0\n",
        "  n = 0\n",
        "  for data in loader:\n",
        "    data = data.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    logs, aux_logs = model(data) \n",
        "    loss = F.nll_loss(logs, data.y)   # TODO: Need to change what data.y is \n",
        "    \n",
        "    if model.use_aux_loss:\n",
        "      aux_loss = F.nll_loss(aux_logs.view(-1, aux_logs.size(-1)), data.y.unsqueeze(0).expand(aux_logs.size(0), -1).clone().view(-1))\n",
        "      loss = 0.75 * loss + 0.25 * aux_loss \n",
        "    \n",
        "    loss.backward()\n",
        "    loss_all += data.num_graphs * loss.item()\n",
        "    n += len(data.y)\n",
        "    optimizer.step()\n",
        "\n",
        "  return loss_all / n"
      ],
      "metadata": {
        "id": "GB8RAp5Lf2YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, loader):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    loss_all = 0\n",
        "    for data in loader:\n",
        "      data = data.to(device)\n",
        "      logs, aux_logs = model(data)\n",
        "      loss_all += F.nll_loss(logs, data.y, reduction=\"sum\").item()\n",
        "  return loss_all / len(loader.dataset)"
      ],
      "metadata": {
        "id": "e36kqtIcpsN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, loader):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "      data = data.to(device)\n",
        "      logs, aux_logs = model(data)\n",
        "      pred = logs.max(1)[1]\n",
        "      correct += pred.eq(data.y).sum().item()\n",
        "  return correct / len(loader.dataset)"
      ],
      "metadata": {
        "id": "wG80t0A3qRdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Dataset into K-Folds"
      ],
      "metadata": {
        "id": "YyriXAImrdTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def separate_data(dataset_len, seed=0):\n",
        "  folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "  idx_list = []\n",
        "  for idx in folds.split(np.zeros(dataset_len), np.zeros(dataset_len)):\n",
        "    idx_list.append(idx)\n",
        "  return idx_list"
      ],
      "metadata": {
        "id": "3wui_6Yerfn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Model"
      ],
      "metadata": {
        "id": "RK_8wBdMgBjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "# Set batch size\n",
        "BATCH = 32    # Default batch size in DropGNN\n",
        "\n",
        "# Set number of epochs\n",
        "NUM_EPOCHS = 350\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")"
      ],
      "metadata": {
        "id": "nQjeCJN-c8og",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe7dd714-7d4d-4de8-a3ad-42e2eacae683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model\n",
        "model = DropGIN(20, 0.1).to(device)\n",
        "\n",
        "# Split dataset\n",
        "n = 400\n",
        "splits = separate_data(n, seed=0)"
      ],
      "metadata": {
        "id": "7qE6ky4lgAFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "acc = []\n",
        "for i, (train_idx, valid_idx) in enumerate(splits[2:3]):\n",
        "  model.reset_parameters()    # Resets upon every epoch (TODO: NEED TO REMOVE)\n",
        "  lr = 0.01\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5) # Used to learn learning rate \n",
        "  \n",
        "  valid_dataset = dataset[valid_idx.tolist()]\n",
        "  train_dataset = dataset[train_idx.tolist()]\n",
        "\n",
        "  valid_loader = DataLoader(valid_dataset, batch_size=BATCH)\n",
        "  train_loader = DataLoader(train_dataset, batch_size=BATCH)\n",
        "\n",
        "  # Train model for multiple epochs\n",
        "  valid_acc = 0 \n",
        "  acc_temp = []\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    if epoch % 25 == 0:\n",
        "      start = time.time()\n",
        "\n",
        "    lr = scheduler.optimizer.param_groups[0]['lr']\n",
        "    train_loss = train(model, epoch, train_loader, optimizer)\n",
        "    scheduler.step()\n",
        "    valid_acc = test(model, valid_loader)\n",
        "    acc_temp.append(valid_acc)\n",
        "\n",
        "    if epoch % 25 == 0:\n",
        "      print('Epoch: {:03d}, LR: {:7f}, Train Loss: {:.7f}, '\n",
        "        'Val Loss: {:.7f}, Test Acc: {:.7f}, Time: {:7f}'.format(\n",
        "          epoch, lr, train_loss, 0, valid_acc, time.time() - start), flush=True)\n",
        "\n",
        "  acc.append(torch.tensor(acc_temp))\n",
        "\n",
        "acc = torch.stack(acc, dim=0)\n",
        "acc_mean = acc.mean(dim=0)\n",
        "best_epoch = acc_mean.argmax().item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diafNwnnsw1d",
        "outputId": "4a95e6a7-31bd-46d4-9020-05c85ab17924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 000, LR: 0.010000, Train Loss: 0.5359498, Val Loss: 0.0000000, Test Acc: 1.0000000, Time: 0.386739\n",
            "Epoch: 025, LR: 0.010000, Train Loss: 0.0000002, Val Loss: 0.0000000, Test Acc: 1.0000000, Time: 0.160006\n",
            "Epoch: 050, LR: 0.005000, Train Loss: 0.0000000, Val Loss: 0.0000000, Test Acc: 1.0000000, Time: 0.149529\n",
            "Epoch: 075, LR: 0.005000, Train Loss: 0.0000001, Val Loss: 0.0000000, Test Acc: 1.0000000, Time: 0.171057\n",
            "Epoch: 100, LR: 0.002500, Train Loss: 0.0000000, Val Loss: 0.0000000, Test Acc: 1.0000000, Time: 0.157632\n",
            "Epoch: 125, LR: 0.002500, Train Loss: 0.0000000, Val Loss: 0.0000000, Test Acc: 1.0000000, Time: 0.165802\n",
            "Epoch: 150, LR: 0.001250, Train Loss: 0.0000001, Val Loss: 0.0000000, Test Acc: 1.0000000, Time: 0.158530\n",
            "Epoch: 175, LR: 0.001250, Train Loss: 0.0001777, Val Loss: 0.0000000, Test Acc: 1.0000000, Time: 0.157275\n",
            "Epoch: 200, LR: 0.000625, Train Loss: 0.0000000, Val Loss: 0.0000000, Test Acc: 1.0000000, Time: 0.165084\n",
            "Epoch: 225, LR: 0.000625, Train Loss: 0.0000001, Val Loss: 0.0000000, Test Acc: 1.0000000, Time: 0.156861\n",
            "Epoch: 250, LR: 0.000313, Train Loss: 0.0000000, Val Loss: 0.0000000, Test Acc: 1.0000000, Time: 0.158370\n",
            "Epoch: 275, LR: 0.000313, Train Loss: 0.0000000, Val Loss: 0.0000000, Test Acc: 1.0000000, Time: 0.156933\n",
            "Epoch: 300, LR: 0.000156, Train Loss: 0.0000002, Val Loss: 0.0000000, Test Acc: 1.0000000, Time: 0.152672\n",
            "Epoch: 325, LR: 0.000156, Train Loss: 0.0000000, Val Loss: 0.0000000, Test Acc: 1.0000000, Time: 0.167127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('---------------- Final Epoch Result ----------------')\n",
        "print('Mean: {:7f}, Std: {:7f}'.format(acc[:,-1].mean(), acc[:,-1].std()))\n",
        "print(f'---------------- Best Epoch: {best_epoch} ----------------')\n",
        "print('Mean: {:7f}, Std: {:7f}'.format(acc[:,best_epoch].mean(), acc[:,best_epoch].std()), flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Uu-W5AzzMoH",
        "outputId": "2dff7bd7-286e-41d3-ac71-2c32ed8f4232"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- Final Epoch Result ----------------\n",
            "Mean: 1.000000, Std:     nan\n",
            "---------------- Best Epoch: 0 ----------------\n",
            "Mean: 1.000000, Std:     nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WyEQvvj204c1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}