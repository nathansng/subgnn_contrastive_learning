# -*- coding: utf-8 -*-
"""DropGNN + Contrastive Learning

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yyx9WIEyti-oEdXQetJBHIS3jfodB_lM

# DropGNN + Contrastive Learning Prototyping

Work based off of DropGNN

https://arxiv.org/pdf/2111.06283.pdf 

https://github.com/KarolisMart/DropGNN
"""

import os.path as osp
import numpy as np
import networkx as nx
import time
import random
import matplotlib.pyplot as plt
import torch
from torch import nn
import torch.nn.functional as F
from sklearn.model_selection import StratifiedKFold

# Torch Geometric 
try: 
  from torch_geometric.data import DataLoader, Data
  from torch_geometric.datasets import TUDataset
  from torch_geometric.utils import degree
  from torch_geometric.utils.convert import from_networkx
  from torch_geometric.nn import GINConv, GINEConv, global_add_pool
except ModuleNotFoundError: 
  !pip install torch_geometric
  from torch_geometric.data import DataLoader, Data
  from torch_geometric.datasets import TUDataset
  from torch_geometric.utils import degree
  from torch_geometric.utils.convert import from_networkx
  from torch_geometric.nn import GINConv, GINEConv, global_add_pool

"""## Generate Synthetic Datasets"""

class SymmetrySet:
    def __init__(self):
        self.hidden_units = 0
        self.num_classes = 0
        self.num_features = 0
        self.num_nodes = 0

    def addports(self, data):
        data.ports = torch.zeros(data.num_edges, 1)
        degs = degree(data.edge_index[0], data.num_nodes, dtype=torch.long) # out degree of all nodes
        for n in range(data.num_nodes):
            deg = degs[n]
            ports = np.random.permutation(int(deg))
            for i, neighbor in enumerate(data.edge_index[1][data.edge_index[0]==n]):
                nb = int(neighbor)
                data.ports[torch.logical_and(data.edge_index[0]==n, data.edge_index[1]==nb), 0] = float(ports[i])
        return data

    def makefeatures(self, data):
        data.x = torch.ones((data.num_nodes, 1))
        data.id = torch.tensor(np.random.permutation(np.arange(data.num_nodes))).unsqueeze(1)
        return data

    def makedata(self):
        pass

class LimitsOne(SymmetrySet):
    def __init__(self):
        super().__init__()
        self.hidden_units = 16
        self.num_classes = 2
        self.num_features = 4
        self.num_nodes = 8
        self.graph_class = False

    def makedata(self):
        n_nodes = 16 # There are two connected components, each with 8 nodes
        
        ports = [1,1,2,2] * 8
        colors = [0, 1, 2, 3] * 4

        y = torch.tensor([0]* 8 + [1] * 8)
        edge_index = torch.tensor([[0,1,1,2, 2,3,3,0, 4,5,5,6, 6,7,7,4, 8,9,9,10,10,11,11,12,12,13,13,14,14,15,15,8], [1,0,2,1, 3,2,0,3, 5,4,6,5, 7,6,4,7, 9,8,10,9,11,10,12,11,13,12,14,13,15,14,8,15]], dtype=torch.long)
        x = torch.zeros((n_nodes, 4))
        x[range(n_nodes), colors] = 1
        
        data = Data(x=x, edge_index=edge_index, y=y)
        data.id = torch.tensor(np.random.permutation(np.arange(n_nodes))).unsqueeze(1)
        data.ports = torch.tensor(ports).unsqueeze(1)
        return [data]

graph = LimitsOne()

data = graph.makedata()[0]
data

# Node feature matrix (num_nodex x num_node_features)
data.x

# Graph connectivity in COO format (2 x num_edges)
# ith node in first row and second row are connected by an edge
data.edge_index

# Graph/node - level ground truth label 
data.y

"""## Import Real Dataset"""

class MyFilter(object):
  def __call__(self, data):
    return data.num_nodes <= 70

class MyPreTransform(object):
  def __call__(self, data):
    data.x = degree(data.edge_index[0], data.num_nodes, dtype=torch.long)
    data.x = F.one_hot(data.x, num_classes=69).to(torch.float)
    return data

# Download data 
path = osp.join(osp.dirname(osp.realpath("./")), 'data', f'IMDB-BINARY')

dataset = TUDataset(
    path, 
    name = "IMDB-BINARY", 
    pre_transform = MyPreTransform(), 
    pre_filter = MyFilter()
)

print(dataset)

"""## Original DropGIN Model

TODO:

- Need to add function to change whether forward uses contrastive loss head or prediction head 
- Can use whether model is in training or evaluation to swap heads
  - Train Contrastive Model - Uses contrastive loss head and updates base model and contrastive loss head 
  - Validate Contrastive Model - Uses contrastive loss head and DOES NOT update anything
  - Train Prediction Model - Uses prediction loss head and update ONLY prediction loss head
  - Testing - Uses prediction loss head and DOES NOT update anything
"""

class DropGIN(nn.Module):
  def __init__(self, hidden_units, dropout, use_aux_loss=True, num_runs=10, p=0.9):
    super(DropGIN, self).__init__()

    # Set starting parameters for model 
    num_features = dataset.num_features # Number of initial features 
    dim = hidden_units # Number of hidden units per layer 
    self.dropout = dropout # Dropout rate 
    self.use_aux_loss = use_aux_loss
    self.num_runs = num_runs
    self.p = p

    # Number of layers in model
    self.num_layers = 4

    self.convs = nn.ModuleList() # Made of num_layers GINConv (linear -> batchnorm1d -> relu -> linear)
    self.bns = nn.ModuleList() # Made of num_layers BatchNorm1d 
    self.fcs = nn.ModuleList() # Made of num_layers + 1 Linear layers mapping from num_features or dim to num_classes

    # Add initial layer from num_features to dim 
    self.convs.append(GINConv(nn.Sequential(nn.Linear(num_features, dim), nn.BatchNorm1d(dim), nn.ReLU(), nn.Linear(dim, dim))))
    self.bns.append(nn.BatchNorm1d(dim))
    self.fcs.append(nn.Linear(num_features, dataset.num_classes))
    self.fcs.append(nn.Linear(dim, dataset.num_classes))

    # Add additional layers from dim to dim 
    for i in range(self.num_layers-1):
      self.convs.append(GINConv(nn.Sequential(nn.Linear(dim, dim), nn.BatchNorm1d(dim), nn.ReLU(), nn.Linear(dim, dim))))
      self.bns.append(nn.BatchNorm1d(dim))
      self.fcs.append(nn.Linear(dim, dataset.num_classes))

    # Use aux_loss for dropGNN: made of num_layers + 1 linear layers 
    # Adds new module list of linear layers 
    if self.use_aux_loss:
      self.aux_fcs = nn.ModuleList()
      self.aux_fcs.append(nn.Linear(num_features, dataset.num_classes))
      for i in range(self.num_layers):
        self.aux_fcs.append(nn.Linear(dim, dataset.num_classes))
        
  def reset_parameters(self):
    # Resets parameters for Linear, GINConv, and BatchNorm1d layers
    for m in self.modules():
      if isinstance(m, nn.Linear):
        m.reset_parameters()
      elif isinstance(m, GINConv):
        m.reset_parameters()
      elif isinstance(m, nn.BatchNorm1d):
        m.reset_parameters()

  def forward(self, data):
    # Note: num_runs in DropGNN is average number of nodes in each graph in dataset
    # Note: p is 2 * 1 / (1 + gamma), but for this project, p is selected to create augmented views 

    x = data.x
    edge_index = data.edge_index
    batch = data.batch
            
    # Do runs in parallel, by repeating the graphs in the batch
    x = x.unsqueeze(0).expand(self.num_runs, -1, -1).clone()   # Flattens features and creates num_runs copy of them 
    drop = torch.bernoulli(torch.ones([x.size(0), x.size(1)], device=x.device) * self.p).bool()   #  Returns a tensor of randomly dropped nodes based on p (p = probability of not dropping) 
    x[drop] = torch.zeros([drop.sum().long().item(), x.size(-1)], device=x.device)  # Drop nodes from data  
    del drop

    # Run augmented subgraph through model 
    outs = [x]  # Used to store view of x after each layer 
    x = x.view(-1, x.size(-1))  # Swap dimensions of data features 
    run_edge_index = edge_index.repeat(1, self.num_runs) + torch.arange(self.num_runs, device=edge_index.device).repeat_interleave(edge_index.size(1)) * (edge_index.max() + 1) # Expand edge_index and augment values
    for i in range(self.num_layers):
      x = self.convs[i](x, run_edge_index)  # Run node features and edge indices through CONV layer 
      x = self.bns[i](x)  # Run resulting values through BatchNorm1d
      x = F.relu(x)   # Run final values through RELU
      outs.append(x.view(self.num_runs, -1, x.size(-1)))    # Rearrange dimensions and append to outs 
    del run_edge_index

    # TODO: Replace this with replacable head for contrastive learning and making predictions 
    # Aggregates results of runs by summing mean and applying random dropout (not dropping out nodes)
    out = None
    for i, x in enumerate(outs):
      x = x.mean(dim=0)
      x = global_add_pool(x, batch)
      x = F.dropout(self.fcs[i](x), p=self.dropout, training=self.training)
      if out is None:
        out = x
      else:
        out += x

    if self.use_aux_loss:
      aux_out = torch.zeros(self.num_runs, out.size(0), out.size(1), device=out.device)
      run_batch = batch.repeat(self.num_runs) + torch.arange(self.num_runs, device=edge_index.device).repeat_interleave(batch.size(0)) * (batch.max() + 1)
      for i, x in enumerate(outs):
        x = x.view(-1, x.size(-1))
        x = global_add_pool(x, run_batch)
        x = x.view(self.num_runs, -1, x.size(-1))
        x = F.dropout(self.aux_fcs[i](x), p=self.dropout, training=self.training)
        aux_out += x

      # Returns probabilities of each class based on aggregated results 
      return F.log_softmax(out, dim=-1), F.log_softmax(aux_out, dim=-1)
    else:
      return F.log_softmax(out, dim=-1), 0

"""## Training Modules

NOTE: Currently, all training modules are for base DropGNN model

TODO: Need 2 training modules

1. Uses contrastive learning loss
2. Uses same loss as DropGNN 
"""

def train(model, epoch, loader, optimizer):
  # Set model to training 
  model.train()

  # Run data through model and update model 
  loss_all = 0
  n = 0
  for data in loader:
    data = data.to(device)
    optimizer.zero_grad()
    logs, aux_logs = model(data) 
    loss = F.nll_loss(logs, data.y)   # TODO: Need to change what data.y is 
    
    if model.use_aux_loss:
      aux_loss = F.nll_loss(aux_logs.view(-1, aux_logs.size(-1)), data.y.unsqueeze(0).expand(aux_logs.size(0), -1).clone().view(-1))
      loss = 0.75 * loss + 0.25 * aux_loss 
    
    loss.backward()
    loss_all += data.num_graphs * loss.item()
    n += len(data.y)
    optimizer.step()

  return loss_all / n

def validate(model, loader):
  model.eval()
  with torch.no_grad():
    loss_all = 0
    for data in loader:
      data = data.to(device)
      logs, aux_logs = model(data)
      loss_all += F.nll_loss(logs, data.y, reduction="sum").item()
  return loss_all / len(loader.dataset)

def test(model, loader):
  model.eval()
  with torch.no_grad():
    correct = 0
    for data in loader:
      data = data.to(device)
      logs, aux_logs = model(data)
      pred = logs.max(1)[1]
      correct += pred.eq(data.y).sum().item()
  return correct / len(loader.dataset)

"""## Split Dataset into K-Folds"""

def separate_data(dataset_len, seed=0):
  folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)
  idx_list = []
  for idx in folds.split(np.zeros(dataset_len), np.zeros(dataset_len)):
    idx_list.append(idx)
  return idx_list

"""## Run Model"""

# Set random seeds
torch.manual_seed(0)
np.random.seed(0)

# Set batch size
BATCH = 32    # Default batch size in DropGNN

# Set number of epochs
NUM_EPOCHS = 350

# Use GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Device: {device}")

# Create model
model = DropGIN(20, 0.1).to(device)

# Split dataset
n = 400
splits = separate_data(n, seed=0)

# Train model
acc = []
for i, (train_idx, valid_idx) in enumerate(splits[2:3]):
  model.reset_parameters()    # Resets upon every epoch (TODO: NEED TO REMOVE)
  lr = 0.01
  optimizer = torch.optim.Adam(model.parameters(), lr=lr)
  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5) # Used to learn learning rate 
  
  valid_dataset = dataset[valid_idx.tolist()]
  train_dataset = dataset[train_idx.tolist()]

  valid_loader = DataLoader(valid_dataset, batch_size=BATCH)
  train_loader = DataLoader(train_dataset, batch_size=BATCH)

  # Train model for multiple epochs
  valid_acc = 0 
  acc_temp = []
  for epoch in range(NUM_EPOCHS):
    if epoch % 25 == 0:
      start = time.time()

    lr = scheduler.optimizer.param_groups[0]['lr']
    train_loss = train(model, epoch, train_loader, optimizer)
    scheduler.step()
    valid_acc = test(model, valid_loader)
    acc_temp.append(valid_acc)

    if epoch % 25 == 0:
      print('Epoch: {:03d}, LR: {:7f}, Train Loss: {:.7f}, '
        'Val Loss: {:.7f}, Test Acc: {:.7f}, Time: {:7f}'.format(
          epoch, lr, train_loss, 0, valid_acc, time.time() - start), flush=True)

  acc.append(torch.tensor(acc_temp))

acc = torch.stack(acc, dim=0)
acc_mean = acc.mean(dim=0)
best_epoch = acc_mean.argmax().item()

print('---------------- Final Epoch Result ----------------')
print('Mean: {:7f}, Std: {:7f}'.format(acc[:,-1].mean(), acc[:,-1].std()))
print(f'---------------- Best Epoch: {best_epoch} ----------------')
print('Mean: {:7f}, Std: {:7f}'.format(acc[:,best_epoch].mean(), acc[:,best_epoch].std()), flush=True)

