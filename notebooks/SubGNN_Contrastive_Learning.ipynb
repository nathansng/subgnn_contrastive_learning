{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bv46oqe2DpeK"
   },
   "source": [
    "# SubGNN & Contrastive Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdzu3FAaGB64"
   },
   "source": [
    "This notebook contains the Subgraph Neural Network Model that uses contrastive learning technique to learn graph embeddings for downstream graph predictions. The subgraph neural network model is inspired by the DropGNN model and has been modified to for easier use via config files. The contrastive learning framework is inspired by the SimCLR contrastive learning framework. \n",
    "\n",
    "DropGNN: \n",
    "- https://arxiv.org/pdf/2111.06283.pdf \n",
    "- https://github.com/KarolisMart/DropGNN \n",
    "\n",
    "SimCLR: \n",
    "- https://arxiv.org/pdf/2002.05709.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "cmcqq2oyHCvQ"
   },
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "# Torch Geometric \n",
    "try: \n",
    "    from torch_geometric.data import DataLoader, Data\n",
    "    from torch_geometric.data.dataloader import Collater\n",
    "    from torch_geometric.datasets import TUDataset\n",
    "    from torch_geometric.utils import degree\n",
    "    from torch_geometric.utils.convert import from_networkx\n",
    "    from torch_geometric.nn import GINConv, GINEConv, global_add_pool\n",
    "except ModuleNotFoundError: \n",
    "    !pip install torch_geometric\n",
    "    from torch_geometric.data import DataLoader, Data\n",
    "    from torch_geometric.data.dataloader import Collater\n",
    "    from torch_geometric.datasets import TUDataset\n",
    "    from torch_geometric.utils import degree\n",
    "    from torch_geometric.utils.convert import from_networkx\n",
    "    from torch_geometric.nn import GINConv, GINEConv, global_add_pool\n",
    "    \n",
    "# Pytorch Metric Learning\n",
    "try: \n",
    "    from pytorch_metric_learning import losses\n",
    "except ModuleNotFoundError:\n",
    "    !pip install pytorch-metric-learning\n",
    "    from pytorch_metric_learning import losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4VpVUxbJeBQ"
   },
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the IMDB-BINARY dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "kpop90ZxIaZW"
   },
   "outputs": [],
   "source": [
    "class MyFilter(object):\n",
    "    def __call__(self, data):\n",
    "        return data.num_nodes <= 70\n",
    "\n",
    "class MyPreTransform(object):\n",
    "    def __call__(self, data):\n",
    "        data.x = degree(data.edge_index[0], data.num_nodes, dtype=torch.long)\n",
    "        data.x = F.one_hot(data.x, num_classes=69).to(torch.float)\n",
    "        return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "oPb5CIfZJ0TV"
   },
   "outputs": [],
   "source": [
    "# Download data \n",
    "path = osp.join(osp.dirname(osp.realpath(\"./\")), 'data', f'IMDB-BINARY')\n",
    "\n",
    "dataset = TUDataset(\n",
    "    path, \n",
    "    name = \"IMDB-BINARY\", \n",
    "    pre_transform = MyPreTransform(), \n",
    "    pre_filter = MyFilter()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w1u0WXdtb_Zj",
    "outputId": "db3223f4-c7a7-4096-aa77-046cfad4b008"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB-BINARY(996)\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the IMDB-MULTI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data \n",
    "path = osp.join(osp.dirname(osp.realpath(\"./\")), 'data', f'IMDB-MULTI')\n",
    "\n",
    "dataset2 = TUDataset(\n",
    "    path, \n",
    "    name = \"IMDB-MULTI\", \n",
    "    pre_transform = MyPreTransform(), \n",
    "    pre_filter = MyFilter()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB-MULTI(1498)\n"
     ]
    }
   ],
   "source": [
    "print(dataset2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SubGNN & Contrastive Learning Model\n",
    "\n",
    "SubGNN Model with Contrastive Learning Techniques using DropGNN subGNN model and SimCLR contrastive learning framework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubGNN_Contrastive(nn.Module):\n",
    "    def __init__(self, num_features, num_reps, num_classes, hidden_units, device, use_aux_loss=True):\n",
    "        super(SubGNN_Contrastive, self).__init__()\n",
    "\n",
    "        # Set starting parameters for model \n",
    "        self.num_features = num_features   # Number of initial features \n",
    "        self.num_reps = num_reps           # Number of features in representation vector \n",
    "        self.num_classes = num_classes     # Number of different classes\n",
    "        self.dim = hidden_units            # Number of units for hidden layers\n",
    "        self.use_aux_loss = use_aux_loss   # Whether to include aux loss to total loss\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        # Number of layers in model\n",
    "        self.num_layers = 4\n",
    "\n",
    "        self.convs = nn.ModuleList()        # Made of num_layers GINConv (linear -> batchnorm1d -> relu -> linear)\n",
    "        self.bns = nn.ModuleList()          # Made of num_layers BatchNorm1d \n",
    "        self.reps = nn.ModuleList()         # Layer between base model and contrastive learning representation\n",
    "        self.fcs = nn.ModuleList()          # Made of num_layers + 1 Linear layers mapping from num_features or dim to num_reps\n",
    "\n",
    "        # Add initial layer from num_features to dim \n",
    "        self.convs.append(GINConv(nn.Sequential(nn.Linear(self.num_features, self.dim), nn.BatchNorm1d(self.dim), nn.ReLU(), nn.Linear(self.dim, self.dim))))\n",
    "        self.bns.append(nn.BatchNorm1d(self.dim))\n",
    "        self.reps.append(nn.Linear(self.num_features, self.num_reps))\n",
    "        self.reps.append(nn.Linear(self.dim, self.num_reps))\n",
    "        self.fcs.append(nn.Linear(self.num_features, self.num_classes))\n",
    "        self.fcs.append(nn.Linear(self.dim, self.num_classes))\n",
    "\n",
    "        # Add additional layers from dim to dim \n",
    "        for i in range(self.num_layers-1):\n",
    "            self.convs.append(GINConv(nn.Sequential(nn.Linear(self.dim, self.dim), nn.BatchNorm1d(self.dim), nn.ReLU(), nn.Linear(self.dim, self.dim))))\n",
    "            self.bns.append(nn.BatchNorm1d(self.dim))\n",
    "            self.reps.append(nn.Linear(self.dim, self.num_reps))\n",
    "            self.fcs.append(nn.Linear(self.dim, self.num_classes))\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        # Resets parameters for Linear, GINConv, and BatchNorm1d layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.reset_parameters()\n",
    "            elif isinstance(m, GINConv):\n",
    "                m.reset_parameters()\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.reset_parameters()\n",
    "                \n",
    "    def forward(self, data, mode=\"test\", p=None, dropout=None, num_runs=20):\n",
    "        # Runs different modes based on whether running contrastive loss or making predictions\n",
    "        if mode == 'contrastive':\n",
    "            return self.contrastive(data, p, num_runs)\n",
    "        else:\n",
    "            return self.prediction(data, p, dropout, num_runs)\n",
    "        \n",
    "    def contrastive(self, data, p, num_runs):\n",
    "        # Trains contrastive model and representation vector model \n",
    "        \n",
    "        # Note: num_runs in DropGNN is average number of nodes in each graph in dataset\n",
    "        # Note: p is 2 * 1 / (1 + gamma), but for this project, p is selected to create augmented views \n",
    "        \n",
    "        self.p = p\n",
    "        self.num_runs = num_runs\n",
    "        \n",
    "        # Store all graphs in sampled batch as one large graph with separate components\n",
    "        x = data.x                     # All nodes and their features (# nodes x # node features)\n",
    "        edge_index = data.edge_index   # All edge index pairs from large single graph\n",
    "        batch = data.batch             # Batch numbers that group nodes within the same graph with same batch number\n",
    "        \n",
    "        # Do runs in parallel by repeating nodes and creating num_runs different views\n",
    "        x = x.unsqueeze(0).expand(self.num_runs, -1, -1).clone()   # Creates num_runs copy of node features\n",
    "        drop = torch.bernoulli(torch.ones([x.size(0), x.size(1)], device=x.device) * self.p).bool()   #  Randomly determine whether node is dropped within each copy of num_runs\n",
    "        x[drop] = torch.zeros([drop.sum().long().item(), x.size(-1)], device=x.device)  # Drop nodes from graphs  \n",
    "        del drop\n",
    "        \n",
    "        # Allow gradients to update base model \n",
    "        if self.training:\n",
    "            for layer in self.convs: \n",
    "                for p in layer.parameters():\n",
    "                    p.requires_grad = True\n",
    "\n",
    "            for layer in self.bns:\n",
    "                for p in layer.parameters():\n",
    "                    p.requires_grad = True\n",
    "        \n",
    "        # Run augmented subgraph through model \n",
    "        outs = [x]  # Used to store n-hop neighborhood representations, after running through model n times\n",
    "        x = x.view(-1, x.size(-1))  # Concat all num_run copies of nodes \n",
    "        run_edge_index = edge_index.repeat(1, self.num_runs) + torch.arange(self.num_runs, device=edge_index.device).repeat_interleave(edge_index.size(1)) * (edge_index.max() + 1) # Transform edge_index to correspond to the same nodes in concatenated form  \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, run_edge_index)  # Run node features and edge indices through CONV layer \n",
    "            x = self.bns[i](x)  # Run resulting values through BatchNorm1d\n",
    "            x = F.relu(x)   # Run final values through RELU\n",
    "            outs.append(x.view(self.num_runs, -1, x.size(-1)))  # Return x back to original stacked form \n",
    "        del run_edge_index\n",
    "        \n",
    "        # Aggregates results of runs by taking mean of each run and summing results of runs\n",
    "        out = None\n",
    "        for i, x in enumerate(outs):\n",
    "            x = x.mean(dim=0)                  # Take average of all node features of same nodes \n",
    "            x = global_add_pool(x, batch)      # Take the sum of all node features for nodes in same graph \n",
    "            x = self.reps[i](x)                # Run graph features into linear layer to get contrastive representation\n",
    "            if out is None:\n",
    "                out = x\n",
    "            else:\n",
    "                out += x\n",
    "                \n",
    "        # Returns all contrastive graph embeddings in batch \n",
    "        return out\n",
    "    \n",
    "    def prediction(self, data, p, dropout, num_runs):\n",
    "        self.p = p\n",
    "        self.dropout = dropout\n",
    "        self.num_runs = num_runs\n",
    "        \n",
    "        # Create intermediate representations \n",
    "        x = data.x \n",
    "        edge_index = data.edge_index\n",
    "        batch = data.batch \n",
    "        \n",
    "        # Do runs in parallel, by repeating the graphs in the batch\n",
    "        x = x.unsqueeze(0).expand(self.num_runs, -1, -1).clone()   # Flattens features and creates num_runs copy of them \n",
    "        drop = torch.bernoulli(torch.ones([x.size(0), x.size(1)], device=x.device) * self.p).bool()   #  Returns a tensor of randomly dropped nodes based on p (p = probability of dropping) \n",
    "        x[drop] = torch.zeros([drop.sum().long().item(), x.size(-1)], device=x.device)  # Drop nodes from data  \n",
    "        del drop\n",
    "        \n",
    "        # Stop gradients from updating base model \n",
    "        for layer in self.convs:\n",
    "            for p in layer.parameters():\n",
    "                p.requires_grad = False\n",
    "                \n",
    "        for layer in self.bns:\n",
    "            for p in layer.parameters():\n",
    "                p.requires_grad = False\n",
    "        \n",
    "        # Run augmented subgraph through model \n",
    "        outs = [x]  # Used to store view of x after each layer \n",
    "        x = x.view(-1, x.size(-1))  # Swap dimensions of data features \n",
    "        run_edge_index = edge_index.repeat(1, self.num_runs) + torch.arange(self.num_runs, device=edge_index.device).repeat_interleave(edge_index.size(1)) * (edge_index.max() + 1) # Expand edge_index and augment values\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, run_edge_index)  # Run node features and edge indices through CONV layer \n",
    "            x = self.bns[i](x)  # Run resulting values through BatchNorm1d\n",
    "            x = F.relu(x)   # Run final values through RELU\n",
    "            outs.append(x.view(self.num_runs, -1, x.size(-1)))    # Rearrange dimensions and append to outs \n",
    "        del run_edge_index\n",
    "        \n",
    "        # Aggregates results of runs by summing mean and applying random dropout (not dropping out nodes)\n",
    "        out = None\n",
    "        for i, x in enumerate(outs):\n",
    "            x = x.mean(dim=0)\n",
    "            x = global_add_pool(x, batch)\n",
    "            x = F.dropout(self.fcs[i](x), p=self.dropout, training=self.training)\n",
    "            if out is None:\n",
    "                out = x\n",
    "            else:\n",
    "                out += x\n",
    "        \n",
    "        # Returns the likelihood of each outcome class\n",
    "        return F.log_softmax(out, dim=-1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fF6Gp8oIf2yR"
   },
   "source": [
    "## Training Modules\n",
    "\n",
    "Training, validation, and testing functions to train and call on Contrastive Learning and for Downstream Graph Prediction using the SubGNN & Contrastive Learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive Learning Training Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to train contrastive model \n",
    "def train_contrastive(model, loader, optimizer, loss_fn, p1=0.1, p2=0.2, device=None):\n",
    "    # Set model to training\n",
    "    model.train()\n",
    "    \n",
    "    # Run data through model and update model\n",
    "    loss_all = 0\n",
    "    n = 0 \n",
    "    for data in loader: \n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        embeddings_1 = model(data, mode = \"contrastive\", p = p1)\n",
    "        embeddings_2 = model(data, mode = \"contrastive\", p = p2)\n",
    "        \n",
    "        # Used as loss(embeddings, labels)\n",
    "        loss = loss_fn(embeddings_1, embeddings_2)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        \n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "        n += data.num_graphs\n",
    "    return loss_all / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to validate contrastive model \n",
    "def valid_contrastive(model, loader, loss_fn, p1=0.1, p2=0.2, device=None):\n",
    "    # Set model to eval\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss_all = 0\n",
    "        n = 0\n",
    "        for data in loader: \n",
    "            data = data.to(device)\n",
    "            embeddings_1 = model(data, mode = \"contrastive\", p = p1)\n",
    "            embeddings_2 = model(data, mode = \"contrastive\", p = p2)\n",
    "            loss = loss_fn(embeddings_1, embeddings_2)\n",
    "            \n",
    "            loss_all += data.num_graphs * loss.item()\n",
    "            n += data.num_graphs\n",
    "    return loss_all / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Training Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to train prediction model AFTER contrastive learning \n",
    "def train_prediction(model, loader, optimizer, p=0.1, dropout=0.5, device=None):\n",
    "    # Set model to training\n",
    "    model.train()\n",
    "    \n",
    "    # Run data through model and update model \n",
    "    loss_all = 0\n",
    "    n = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        results = model(data, mode = \"prediction\", p = p, dropout = dropout)\n",
    "        loss = F.nll_loss(results, data.y)\n",
    "    \n",
    "        loss.backward()\n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "        n += len(data.y)\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss_all / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to validate prediction model AFTER contrastive learning, returns loss \n",
    "def valid_prediction(model, loader, p=0.1, dropout=0.5, device=None):\n",
    "    # Set model to eval\n",
    "    model.eval()\n",
    "    \n",
    "    # Run data through model\n",
    "    with torch.no_grad():\n",
    "        loss_all = 0\n",
    "        n = 0\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            results = model(data, mode = \"prediction\", p = p, dropout = dropout)\n",
    "            loss = F.nll_loss(results, data.y)\n",
    "                \n",
    "            loss_all += data.num_graphs * loss.item()\n",
    "            n += len(data.y)\n",
    "\n",
    "    return loss_all / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to test prediction model AFTER contrastive learning, returns accuracy\n",
    "def test_prediction(model, loader, p=0.1, dropout=0.5, device=None):\n",
    "    # Set model to eval\n",
    "    model.eval() \n",
    "    \n",
    "    # Run data through model and make predictions\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        for data in loader: \n",
    "            data = data.to(device)\n",
    "            results = model(data, mode = \"prediction\", p = p, dropout = dropout)\n",
    "            pred = results.max(1)[1]\n",
    "            correct += pred.eq(data.y).sum().item()\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyriXAImrdTd"
   },
   "source": [
    "## Split Dataset into K-Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "3wui_6Yerfn-"
   },
   "outputs": [],
   "source": [
    "def separate_data(dataset_len, seed=0, n_splits=10):\n",
    "    folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    idx_list = []\n",
    "    for idx in folds.split(np.zeros(dataset_len), np.zeros(dataset_len)):\n",
    "        idx_list.append(idx)\n",
    "    return idx_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation Loop\n",
    "\n",
    "Automates training process by training both contrastive learning and graph prediction and making finals predictions using final model. Also automates the evaluation process of a model to test its performance using k-folds train-test splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, dataset, train_idx, test_idx, batch_size, epochs, p1, p2, p, dropout, device, lr=0.001, seed=0, m=10, filename=None):\n",
    "    \"\"\"\n",
    "    Runs a single training loop based on given training and testing indices \n",
    "    \"\"\"\n",
    "    # Set random seeds \n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Set batch size and number of epochs \n",
    "    BATCH = batch_size\n",
    "    NUM_EPOCHS = epochs \n",
    "    LR = lr\n",
    "    \n",
    "    # Create training and testing datasets\n",
    "    train_dataset = dataset[train_idx.tolist()]\n",
    "    test_dataset = dataset[test_idx.tolist()]\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, sampler=torch.utils.data.RandomSampler(train_dataset, replacement=True, num_samples=int(len(train_dataset)*50/(len(train_dataset)/BATCH))), batch_size=BATCH, drop_last=False, collate_fn=Collater(follow_batch=[],exclude_keys=[]))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH)\n",
    "    \n",
    "    # Set up for contrastive learning\n",
    "    loss_func = losses.SelfSupervisedLoss(losses.NTXentLoss())   # Specify contrastive loss function to use \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)      # Optimizer for model to use \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5) # Used to adjust learning rate while training \n",
    "    \n",
    "    # CONTRASTIVE LEARNING: Train model on contrastive representation \n",
    "    print(\"STARTING CONTRASTIVE LEARNING\")\n",
    "    if filename != None:\n",
    "        with open(filename, \"a\") as f: \n",
    "            print(\"STARTING CONTRASTIVE LEARNING\", file=f)\n",
    "    \n",
    "    contrastive_losses = []\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        if epoch % m == 0:\n",
    "            start = time.time()\n",
    "\n",
    "        lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "        train_loss = train_contrastive(model, train_loader, optimizer, loss_func, p1=p1, p2=p2, device=device)\n",
    "        scheduler.step()\n",
    "        test_loss = valid_contrastive(model, test_loader, loss_func, p1=p1, p2=p2, device=device)\n",
    "        contrastive_losses.append(test_loss)\n",
    "\n",
    "        if epoch % m == 0:\n",
    "            print('Epoch: {:03d}, LR: {:7f}, Train Loss: {:.7f}, '\n",
    "                'Val Loss: {:.7f}, Time: {:7f}'.format(\n",
    "                    epoch, lr, train_loss, test_loss, time.time() - start), flush=True)\n",
    "            if filename != None:\n",
    "                with open(filename, \"a\") as f: \n",
    "                    print('Epoch: {:03d}, LR: {:7f}, Train Loss: {:.7f}, '\n",
    "                        'Val Loss: {:.7f}, Time: {:7f}'.format(\n",
    "                            epoch, lr, train_loss, test_loss, time.time() - start), flush=True, file=f)\n",
    "            \n",
    "    # Set up for prediction \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)      # Optimizer for model to use \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5) # Used to adjust learning rate while training \n",
    "    \n",
    "    # PREDICTION: Train model using contrastive representations to make predictions \n",
    "    print(\"\\nSTARTING PREDICTION LEARNING\")\n",
    "    if filename != None: \n",
    "        with open(filename, \"a\") as f: \n",
    "            print(\"\\nSTARTING PREDICTION LEARNING\", file=f)\n",
    "    \n",
    "    prediction_losses = []\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        if epoch % m == 0:\n",
    "            start = time.time()\n",
    "\n",
    "        lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "        train_loss = train_prediction(model, train_loader, optimizer, p=p, dropout=dropout, device=device)\n",
    "        scheduler.step()\n",
    "        test_loss = valid_prediction(model, test_loader, p=p, dropout=dropout, device=device)\n",
    "        prediction_losses.append(test_loss)\n",
    "\n",
    "        if epoch % m == 0:\n",
    "            print('Epoch: {:03d}, LR: {:7f}, Train Loss: {:.7f}, '\n",
    "                'Val Loss: {:.7f}, Time: {:7f}'.format(\n",
    "                    epoch, lr, train_loss, test_loss, time.time() - start), flush=True)\n",
    "            if filename != None:\n",
    "                with open(filename, \"a\") as f: \n",
    "                    print('Epoch: {:03d}, LR: {:7f}, Train Loss: {:.7f}, '\n",
    "                        'Val Loss: {:.7f}, Time: {:7f}'.format(\n",
    "                            epoch, lr, train_loss, test_loss, time.time() - start), flush=True, file=f)\n",
    "            \n",
    "    # Test final accuracy of final model \n",
    "    test_acc = test_prediction(model, test_loader, dropout=dropout, device=device)\n",
    "    print(f\"\\nFinal Prediction Accuracy: {test_acc}\\n\")\n",
    "    if filename != None: \n",
    "        with open(filename, \"a\") as f: \n",
    "            print(f\"\\nFinal Prediction Accuracy: {test_acc}\\n\", file=f)\n",
    "    \n",
    "    return contrastive_losses, prediction_losses, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_loop(model, dataset, splits, batch_size, epochs, p1, p2, p, dropout, device, lr=0.001, seed=0, m=10, filename=None):\n",
    "    # Train model on different splits, meant to evaluate model, not save best model\n",
    "    contrastive_loss = []\n",
    "    prediction_loss = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    # Train a new model on every fold for evaluation\n",
    "    for i, (train_idx, test_idx) in enumerate(splits): \n",
    "        print(f\"Running Split {i}\")\n",
    "        if filename != None: \n",
    "            with open(filename, \"a\") as f: \n",
    "                print(f\"Running Split {i}\", file=f)\n",
    "        \n",
    "        model.reset_parameters()    # Resets upon every new fold \n",
    "        c_loss, p_loss, t_acc = training_loop(model, dataset, train_idx, test_idx, batch_size, epochs, p1, p2, p, dropout, device, lr, seed, m, filename)\n",
    "        contrastive_loss.append(torch.tensor(c_loss))\n",
    "        prediction_loss.append(torch.tensor(p_loss))\n",
    "        test_accuracies.append(t_acc)\n",
    "        \n",
    "    # Calculate average contrastive loss and return best epoch for contrastive loss\n",
    "    contrastive_loss = torch.stack(contrastive_loss, dim=0)\n",
    "    contrastive_loss_mean = contrastive_loss.mean(dim=0)\n",
    "    best_contrastive_epoch = contrastive_loss_mean.argmin()\n",
    "\n",
    "    # Calculate average prediction loss and return best epoch for predictions \n",
    "    prediction_loss = torch.stack(prediction_loss, dim=0)\n",
    "    prediction_loss_mean = prediction_loss.mean(dim=0)\n",
    "    best_prediction_epoch = prediction_loss_mean.argmin()\n",
    "    \n",
    "    # Print average final prediction accuracy\n",
    "    test_accuracies = torch.tensor(test_accuracies)\n",
    "    print(f\"Average Test Accuracy: {test_accuracies.mean()}\\n\")\n",
    "    if filename != None: \n",
    "        with open(filename, \"a\") as f: \n",
    "            print(f\"Average Test Accuracy: {test_accuracies.mean()}\\n\", file=f)\n",
    "    \n",
    "    return (contrastive_loss, contrastive_loss_mean, best_contrastive_epoch), (prediction_loss, prediction_loss_mean, best_prediction_epoch), test_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DropGNN + Contrastive Learning Model\n",
    "\n",
    "Examples of how to train and use the SubGNN & Contrastive Learning model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Experiments \n",
    "\n",
    "Runs different dropout probabilities and trains the model on the dataset. All results are both printed and saved to a text file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test IMDB-Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_reps = [32]\n",
    "p1 = [0.1, 0.4, 0.7]\n",
    "p2 = [0.25, 0.55, 0.85]\n",
    "p = [0.0, 0.1, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for n in num_reps:\n",
    "    for p_1 in p1: \n",
    "        for p_2 in p2: \n",
    "            for p_i in p:\n",
    "                # Set random seeds\n",
    "                torch.manual_seed(0)\n",
    "                np.random.seed(0)\n",
    "                \n",
    "                filename = f\"results/test_{i}.txt\"\n",
    "                \n",
    "                with open(filename, \"a\") as f: \n",
    "                    print(f\"num_reps: {n}\\np1: {p_1}\\np2: {p_2}\\np: {p_i}\\n\", file=f)\n",
    "                \n",
    "                model_cfg = {\n",
    "                    \"num_features\": dataset.num_features, \n",
    "                    \"num_classes\": dataset.num_classes, \n",
    "                    \"num_reps\": n, \n",
    "                    \"hidden_units\": 64\n",
    "                }\n",
    "                \n",
    "                train_cfg = {\n",
    "                    \"model\": SubGNN_Contrastive(**model_cfg).to(device),\n",
    "                    \"dataset\": dataset, \n",
    "                    \"splits\": separate_data(len(dataset), seed=0, n_splits=3),\n",
    "                    \"batch_size\": 32,\n",
    "                    \"epochs\": 101, \n",
    "                    \"p1\": p_1, \n",
    "                    \"p2\": p_2, \n",
    "                    \"p\": p_i,\n",
    "                    \"dropout\": 0.1, \n",
    "                    \"device\": device, \n",
    "                    \"lr\": 0.001, \n",
    "                    \"seed\": 0, \n",
    "                    \"m\": 10, \n",
    "                    \"filename\": filename\n",
    "                }\n",
    "                \n",
    "                eval_results = evaluation_loop(**train_cfg)\n",
    "                \n",
    "                result_filename = f\"results/test_{i}_metrics.txt\"\n",
    "                with open(result_filename, \"a\") as f: \n",
    "                    print(eval_results, file=f)\n",
    "                \n",
    "                i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test IMDB-Multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_reps = [32]\n",
    "p1 = [0.1, 0.4]\n",
    "p2 = [0.25, 0.55]\n",
    "p = [0.0, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for n in num_reps:\n",
    "    for p_1 in p1: \n",
    "        for p_2 in p2: \n",
    "            for p_i in p:\n",
    "                # Set random seeds\n",
    "                torch.manual_seed(0)\n",
    "                np.random.seed(0)\n",
    "                \n",
    "                filename = f\"results2/test_{i}.txt\"\n",
    "                \n",
    "                with open(filename, \"a\") as f: \n",
    "                    print(f\"num_reps: {n}\\np1: {p_1}\\np2: {p_2}\\np: {p_i}\\n\", file=f)\n",
    "                \n",
    "                model_cfg = {\n",
    "                    \"num_features\": dataset2.num_features, \n",
    "                    \"num_classes\": dataset2.num_classes, \n",
    "                    \"num_reps\": n, \n",
    "                    \"hidden_units\": 64\n",
    "                }\n",
    "                \n",
    "                train_cfg = {\n",
    "                    \"model\": SubGNN_Contrastive(**model_cfg).to(device),\n",
    "                    \"dataset\": dataset2, \n",
    "                    \"splits\": separate_data(len(dataset2), seed=0, n_splits=3),\n",
    "                    \"batch_size\": 32,\n",
    "                    \"epochs\": 101, \n",
    "                    \"p1\": p_1, \n",
    "                    \"p2\": p_2, \n",
    "                    \"p\": p_i,\n",
    "                    \"dropout\": 0.1, \n",
    "                    \"device\": device, \n",
    "                    \"lr\": 0.001, \n",
    "                    \"seed\": 0, \n",
    "                    \"m\": 10, \n",
    "                    \"filename\": filename\n",
    "                }\n",
    "                \n",
    "                eval_results = evaluation_loop(**train_cfg)\n",
    "                \n",
    "                result_filename = f\"results2/test_{i}_metrics.txt\"\n",
    "                with open(result_filename, \"a\") as f: \n",
    "                    print(eval_results, file=f)\n",
    "                \n",
    "                i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluation Loop w/ Configs\n",
    "\n",
    "Demonstration of how to run the evaluation loop to evaluate the model using configurations. Can be down in the form of a dictionary or a JSON file that is read in as a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "model_cfg = {\n",
    "    \"num_features\": dataset.num_features, \n",
    "    \"num_classes\": dataset.num_classes, \n",
    "    \"num_reps\": 32, \n",
    "    \"hidden_units\": 64\n",
    "}\n",
    "\n",
    "train_cfg = {\n",
    "    \"model\": SubGNN_Contrastive(**model_cfg).to(device),\n",
    "    \"dataset\": dataset, \n",
    "    \"splits\": separate_data(len(dataset), seed=0, n_splits=5),\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 101, \n",
    "    \"p1\": 0.1, \n",
    "    \"p2\": 0.2, \n",
    "    \"p\": 0.0,\n",
    "    \"dropout\": 0.1, \n",
    "    \"device\": device, \n",
    "    \"lr\": 0.001, \n",
    "    \"seed\": 0, \n",
    "    \"m\": 10, \n",
    "    \"filename\": \"test_1_results.txt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = evaluation_loop(**train_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually set each parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model on IMDB-Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SET UP PARAMETERS FOR TRAINING\n",
    "\"\"\"\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set batch size\n",
    "BATCH = 32    # Default batch size in DropGNN\n",
    "\n",
    "# Set number of epochs\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "# Set size of contrastive representation\n",
    "NUM_REPS = 32\n",
    "\n",
    "# Set node dropout probabilities \n",
    "p1 = 0.1\n",
    "p2 = 0.2\n",
    "\n",
    "# Set embedding dropout probabilities \n",
    "dropout = 0.5\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CREATE MODEL AND CREATE DATA SPLITS\n",
    "\"\"\"\n",
    "\n",
    "# Create model\n",
    "model = SubGNN_Contrastive(num_features=dataset.num_features, num_reps=NUM_REPS, num_classes=dataset.num_classes, hidden_units=64).to(device)\n",
    "\n",
    "# Split dataset\n",
    "n = len(dataset)\n",
    "splits = separate_data(n, seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one test of training a single split \n",
    "con_loss, pred_loss, test_acc = training_loop(model, dataset, train_idx=splits[0][0], test_idx=splits[0][0], batch_size=BATCH, epochs=NUM_EPOCHS, p1=p1, p2=p2, dropout=dropout, device=device, lr=0.001, seed=0, m=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluate model performance over several different splits\n",
    "eval_results = evaluation_loop(model, dataset, splits[:2], batch_size=BATCH, epochs=NUM_EPOCHS, p1=p1, p2=p2, dropout=dropout, device=device, lr=0.001, seed=0, m=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model on IMDB-Multi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SET UP PARAMETERS FOR TRAINING\n",
    "\"\"\"\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set batch size\n",
    "BATCH = 32 \n",
    "\n",
    "# Set number of epochs\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "# Set size of contrastive representation \n",
    "NUM_REPS = 16\n",
    "\n",
    "# Set node dropout probabilities \n",
    "p1 = 0.1\n",
    "p2 = 0.2\n",
    "\n",
    "# Set embedding dropout probabilities \n",
    "dropout = 0.1\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CREATE MODEL AND CREATE DATA SPLITS\n",
    "\"\"\"\n",
    "\n",
    "# Create model\n",
    "model2 = SubGNN_Contrastive(num_features=dataset2.num_features, num_reps=NUM_REPS, num_classes=dataset2.num_classes, hidden_units=64).to(device)\n",
    "\n",
    "# Split dataset\n",
    "n = len(dataset2)\n",
    "splits = separate_data(n, seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one test of training a single split \n",
    "con_loss2, pred_loss2, test_acc2 = training_loop(model3, dataset2, train_idx=splits[0][0], test_idx=splits[0][0], batch_size=BATCH, epochs=NUM_EPOCHS, p1=p1, p2=p2, dropout=dropout, device=device, lr=0.0001, seed=0, m=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluate model performance over several different splits\n",
    "eval_results2 = evaluation_loop(model2, dataset2, splits, batch_size=BATCH, epochs=NUM_EPOCHS, p1=p1, p2=p2, dropout=dropout, device=device, lr=0.001, seed=0, m=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SET UP PARAMETERS FOR TRAINING\n",
    "\"\"\"\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set batch size\n",
    "BATCH = 32 \n",
    "\n",
    "# Set number of epochs\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "# Set size of contrastive representation \n",
    "NUM_REPS = 64\n",
    "\n",
    "# Set node dropout probabilities \n",
    "p1 = 0.1\n",
    "p2 = 0.2\n",
    "\n",
    "# Set embedding dropout probabilities \n",
    "dropout = 0.5\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CREATE MODEL AND CREATE DATA SPLITS\n",
    "\"\"\"\n",
    "\n",
    "# Create model\n",
    "model3 = SubGNN_Contrastive(num_features=dataset2.num_features, num_reps=NUM_REPS, num_classes=dataset2.num_classes, hidden_units=64).to(device)\n",
    "\n",
    "# Split dataset\n",
    "n = len(dataset2)\n",
    "splits = separate_data(n, seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one test of training a single split \n",
    "con_loss3, pred_loss3, test_acc3 = training_loop(model3, dataset2, train_idx=splits[0][0], test_idx=splits[0][0], batch_size=BATCH, epochs=NUM_EPOCHS, p1=p1, p2=p2, dropout=dropout, device=device, lr=0.001, seed=0, m=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance over several different splits\n",
    "eval_results3 = evaluation_loop(model3, dataset2, splits, batch_size=BATCH, epochs=NUM_EPOCHS, p1=p1, p2=p2, dropout=dropout, device=device, lr=0.001, seed=0, m=10)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "my-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
